package main

import (
	"context"
	"database/sql"
	"fmt"
	"slices"
	"strings"

	"github.com/estuary/connectors/sqlcapture"
	"github.com/invopop/jsonschema"
	log "github.com/sirupsen/logrus"
)

// discoveryOptions controls how discovery queries are constructed.
type discoveryOptions struct {
	DiscoverOnlyEnabled bool // Only discover tables with Change Tracking enabled
}

// DiscoverTables queries the database for information about tables available for capture.
func (db *sqlserverDatabase) DiscoverTables(ctx context.Context) (map[sqlcapture.StreamID]*sqlcapture.DiscoveryInfo, error) {
	var opts = discoveryOptions{
		DiscoverOnlyEnabled: !db.config.Advanced.DiscoverNonEnabled,
	}

	// Get lists of all tables, columns and primary keys in the database
	var tables, err = getTables(ctx, db.conn, opts)
	if err != nil {
		return nil, fmt.Errorf("unable to list database tables: %w", err)
	}
	columns, err := getColumns(ctx, db.conn, opts)
	if err != nil {
		return nil, fmt.Errorf("unable to list database columns: %w", err)
	}
	primaryKeys, err := getPrimaryKeys(ctx, db.conn, opts)
	if err != nil {
		return nil, fmt.Errorf("unable to list database primary keys: %w", err)
	}
	computedColumns, err := getComputedColumns(ctx, db.conn, opts)
	if err != nil {
		return nil, fmt.Errorf("unable to list database computed columns: %w", err)
	}

	// Aggregate column information into DiscoveryInfo structs using a map
	// from fully-qualified table names to the corresponding info.
	var tableMap = make(map[sqlcapture.StreamID]*sqlcapture.DiscoveryInfo)
	for _, table := range tables {
		var streamID = sqlcapture.JoinStreamID(table.Schema, table.Name)

		// Depending on feature flag settings, we may normalize multiple table names
		// to the same StreamID. This is a problem and other parts of discovery won't
		// be able to handle it gracefully, so it's a fatal error.
		if other, ok := tableMap[streamID]; ok {
			return nil, fmt.Errorf("table name collision between %q and %q",
				fmt.Sprintf("%s.%s", table.Schema, table.Name),
				fmt.Sprintf("%s.%s", other.Schema, other.Name),
			)
		}

		table.UseSchemaInference = true
		table.EmitSourcedSchemas = true
		tableMap[streamID] = table
	}
	for _, column := range columns {
		var streamID = sqlcapture.JoinStreamID(column.TableSchema, column.TableName)
		var info, ok = tableMap[streamID]
		if !ok {
			continue
		}

		if info.Columns == nil {
			info.Columns = make(map[string]sqlcapture.ColumnInfo)
		}
		info.Columns[column.Name] = column
		info.ColumnNames = append(info.ColumnNames, column.Name)
		tableMap[streamID] = info
	}

	// Add primary-key information to the tables map
	for id, key := range primaryKeys {
		var info, ok = tableMap[id]
		if !ok {
			continue
		}
		info.PrimaryKey = key
		tableMap[id] = info
	}

	// Add computed columns information (for informational purposes only - CT can capture computed columns)
	for streamID, table := range tableMap {
		if details, ok := table.ExtraDetails.(*sqlserverTableDiscoveryDetails); ok {
			details.ComputedColumns = computedColumns[streamID]
		}
	}

	// Determine whether the database sorts the keys of each table in a
	// predictable order or not. The term "predictable" here specifically
	// means "able to be reproduced using bytewise lexicographic ordering of
	// the serialized row keys generated by this connector".
	for _, info := range tableMap {
		for _, colName := range info.PrimaryKey {
			var dataType = info.Columns[colName].DataType
			if _, ok := dataType.(*sqlserverTextColumnType); ok {
				info.UnpredictableKeyOrdering = true
			} else if dataType == "numeric" || dataType == "decimal" {
				info.UnpredictableKeyOrdering = true
			}
		}
	}

	// Change Tracking requires primary keys. Mark tables without a primary key as omitted.
	for streamID, info := range tableMap {
		if len(info.PrimaryKey) == 0 {
			log.WithField("table", streamID).Debug("omitting table without primary key (required for Change Tracking)")
			info.OmitBinding = true
		}
	}

	if log.IsLevelEnabled(log.DebugLevel) {
		for id, info := range tableMap {
			log.WithFields(log.Fields{
				"stream":     id,
				"keyColumns": info.PrimaryKey,
			}).Debug("discovered table")
		}
	}

	return tableMap, nil
}

type sqlserverTableDiscoveryDetails struct {
	ComputedColumns []string // List of the names of computed columns in this table, in no particular order.
}

func queryDiscoverTables(opts discoveryOptions) string {
	if opts.DiscoverOnlyEnabled {
		return `
SELECT DISTINCT IST.TABLE_SCHEMA, IST.TABLE_NAME, IST.TABLE_TYPE
  FROM sys.change_tracking_tables ctt
  JOIN sys.tables tbl ON ctt.object_id = tbl.object_id
  JOIN sys.schemas sch ON tbl.schema_id = sch.schema_id
  JOIN INFORMATION_SCHEMA.TABLES IST
    ON IST.TABLE_SCHEMA = sch.name AND IST.TABLE_NAME = tbl.name
  WHERE IST.TABLE_SCHEMA NOT IN ('INFORMATION_SCHEMA', 'PERFORMANCE_SCHEMA', 'SYS')
    AND IST.TABLE_NAME != 'SYSTRANSCHEMAS';`
	}
	return `
  SELECT TABLE_SCHEMA, TABLE_NAME, TABLE_TYPE
  FROM INFORMATION_SCHEMA.TABLES
  WHERE TABLE_SCHEMA != 'INFORMATION_SCHEMA' AND TABLE_SCHEMA != 'PERFORMANCE_SCHEMA'
    AND TABLE_SCHEMA != 'SYS'
	AND TABLE_NAME != 'SYSTRANSCHEMAS';`
}

func getTables(ctx context.Context, conn *sql.DB, opts discoveryOptions) ([]*sqlcapture.DiscoveryInfo, error) {
	rows, err := conn.QueryContext(ctx, queryDiscoverTables(opts))
	if err != nil {
		return nil, fmt.Errorf("error listing tables: %w", err)
	}
	defer rows.Close()

	var tables []*sqlcapture.DiscoveryInfo
	for rows.Next() {
		var tableSchema, tableName, tableType string
		if err := rows.Scan(&tableSchema, &tableName, &tableType); err != nil {
			return nil, fmt.Errorf("error scanning result row: %w", err)
		}
		tables = append(tables, &sqlcapture.DiscoveryInfo{
			Schema:       tableSchema,
			Name:         tableName,
			BaseTable:    strings.EqualFold(tableType, "BASE TABLE"),
			ExtraDetails: &sqlserverTableDiscoveryDetails{},
		})
	}
	return tables, nil
}

func queryDiscoverColumns(opts discoveryOptions) string {
	if opts.DiscoverOnlyEnabled {
		return `
SELECT DISTINCT ISC.TABLE_SCHEMA, ISC.TABLE_NAME, ISC.ORDINAL_POSITION, ISC.COLUMN_NAME,
       ISC.IS_NULLABLE, ISC.DATA_TYPE, ISC.COLLATION_NAME, ISC.CHARACTER_MAXIMUM_LENGTH
  FROM sys.change_tracking_tables ctt
  JOIN sys.tables tbl ON ctt.object_id = tbl.object_id
  JOIN sys.schemas sch ON tbl.schema_id = sch.schema_id
  JOIN INFORMATION_SCHEMA.COLUMNS ISC
    ON ISC.TABLE_SCHEMA = sch.name AND ISC.TABLE_NAME = tbl.name
  ORDER BY ISC.TABLE_SCHEMA, ISC.TABLE_NAME, ISC.ORDINAL_POSITION;`
	}
	return `
  SELECT TABLE_SCHEMA, TABLE_NAME, ORDINAL_POSITION, COLUMN_NAME, IS_NULLABLE, DATA_TYPE, COLLATION_NAME, CHARACTER_MAXIMUM_LENGTH
  FROM INFORMATION_SCHEMA.COLUMNS
  ORDER BY TABLE_SCHEMA, TABLE_NAME, ORDINAL_POSITION;`
}

func getColumns(ctx context.Context, conn *sql.DB, opts discoveryOptions) ([]sqlcapture.ColumnInfo, error) {
	var rows, err = conn.QueryContext(ctx, queryDiscoverColumns(opts))
	if err != nil {
		return nil, fmt.Errorf("error querying columns: %w", err)
	}
	defer rows.Close()

	var columns []sqlcapture.ColumnInfo
	for rows.Next() {
		var ci sqlcapture.ColumnInfo
		var isNullable, typeName string
		var collationName *string
		var maxCharLength *int
		if err := rows.Scan(&ci.TableSchema, &ci.TableName, &ci.Index, &ci.Name, &isNullable, &typeName, &collationName, &maxCharLength); err != nil {
			return nil, fmt.Errorf("error scanning result row: %w", err)
		}
		ci.IsNullable = isNullable != "NO"
		if slices.Contains([]string{"char", "varchar", "nchar", "nvarchar", "text", "ntext"}, typeName) {
			// The collation name should never be null for a text column type, but if it
			// is we'll just default to the string 'NULL' so it's clear what the error is.
			var collation = "NULL"
			if collationName != nil {
				collation = *collationName
			}
			var fullType = typeName
			var columnSize int = 64 // For historical reasons we want to overestimate if CHAR_MAX_LENGTH is null
			if maxCharLength != nil {
				columnSize = *maxCharLength
			}
			if slices.Contains([]string{"char", "varchar", "nchar", "nvarchar"}, typeName) {
				fullType = fmt.Sprintf("%s(%d)", typeName, columnSize)
			}
			ci.DataType = &sqlserverTextColumnType{
				Type:      typeName,
				Collation: collation,
				FullType:  fullType,
				MaxLength: columnSize,
			}
		} else if slices.Contains([]string{"binary", "varbinary"}, typeName) {
			var columnSize int
			if maxCharLength != nil {
				columnSize = *maxCharLength
			}
			ci.DataType = &sqlserverBinaryColumnType{
				Type:      typeName,
				MaxLength: columnSize,
			}
		} else {
			ci.DataType = typeName
		}
		columns = append(columns, ci)
	}
	return columns, nil
}

type sqlserverTextColumnType struct {
	Type      string // The basic type of the column (char / varchar / nchar / nvarchar / text / ntext)
	Collation string // The collation used by the column
	FullType  string // The full type of the column, such as `varchar(32)` for example (used for backfill queries)
	MaxLength int    // The exact maximum length of the column
}

func (t sqlserverTextColumnType) String() string {
	return t.FullType
}

type sqlserverBinaryColumnType struct {
	Type      string // The basic type of the column (binary / varbinary)
	MaxLength int    // The exact maximum length of the column in bytes
}

func (t sqlserverBinaryColumnType) String() string {
	return fmt.Sprintf("%s(%d)", t.Type, t.MaxLength)
}

// Joining on the 6-tuple {CONSTRAINT,TABLE}_{CATALOG,SCHEMA,NAME} is probably
// overkill but shouldn't hurt, and helps to make absolutely sure that we're
// matching up the constraint type with the column names/positions correctly.
func queryDiscoverPrimaryKeys(opts discoveryOptions) string {
	if opts.DiscoverOnlyEnabled {
		return `
SELECT DISTINCT KCU.TABLE_SCHEMA, KCU.TABLE_NAME, KCU.COLUMN_NAME, KCU.ORDINAL_POSITION
  FROM sys.change_tracking_tables ctt
  JOIN sys.tables tbl ON ctt.object_id = tbl.object_id
  JOIN sys.schemas sch ON tbl.schema_id = sch.schema_id
  JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE KCU
    ON KCU.TABLE_SCHEMA = sch.name AND KCU.TABLE_NAME = tbl.name
  JOIN INFORMATION_SCHEMA.TABLE_CONSTRAINTS TCS
    ON  TCS.CONSTRAINT_CATALOG = KCU.CONSTRAINT_CATALOG
    AND TCS.CONSTRAINT_SCHEMA = KCU.CONSTRAINT_SCHEMA
    AND TCS.CONSTRAINT_NAME = KCU.CONSTRAINT_NAME
    AND TCS.TABLE_CATALOG = KCU.TABLE_CATALOG
    AND TCS.TABLE_SCHEMA = KCU.TABLE_SCHEMA
    AND TCS.TABLE_NAME = KCU.TABLE_NAME
  WHERE TCS.CONSTRAINT_TYPE = 'PRIMARY KEY'
  ORDER BY KCU.TABLE_SCHEMA, KCU.TABLE_NAME, KCU.ORDINAL_POSITION;`
	}
	return `
SELECT KCU.TABLE_SCHEMA, KCU.TABLE_NAME, KCU.COLUMN_NAME, KCU.ORDINAL_POSITION
  FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE KCU
  JOIN INFORMATION_SCHEMA.TABLE_CONSTRAINTS TCS
    ON  TCS.CONSTRAINT_CATALOG = KCU.CONSTRAINT_CATALOG
    AND TCS.CONSTRAINT_SCHEMA = KCU.CONSTRAINT_SCHEMA
    AND TCS.CONSTRAINT_NAME = KCU.CONSTRAINT_NAME
    AND TCS.TABLE_CATALOG = KCU.TABLE_CATALOG
    AND TCS.TABLE_SCHEMA = KCU.TABLE_SCHEMA
    AND TCS.TABLE_NAME = KCU.TABLE_NAME
  WHERE TCS.CONSTRAINT_TYPE = 'PRIMARY KEY'
  ORDER BY KCU.TABLE_SCHEMA, KCU.TABLE_NAME, KCU.ORDINAL_POSITION;`
}

func getPrimaryKeys(ctx context.Context, conn *sql.DB, opts discoveryOptions) (map[sqlcapture.StreamID][]string, error) {
	var rows, err = conn.QueryContext(ctx, queryDiscoverPrimaryKeys(opts))
	if err != nil {
		return nil, fmt.Errorf("error querying primary keys: %w", err)
	}
	defer rows.Close()

	var keys = make(map[sqlcapture.StreamID][]string)
	for rows.Next() {
		var tableSchema, tableName, columnName string
		var index int
		if err := rows.Scan(&tableSchema, &tableName, &columnName, &index); err != nil {
			return nil, fmt.Errorf("error scanning result row: %w", err)
		}
		var streamID = sqlcapture.JoinStreamID(tableSchema, tableName)
		keys[streamID] = append(keys[streamID], columnName)
		if index != len(keys[streamID]) {
			return nil, fmt.Errorf("primary key column %q (of table %q) appears out of order", columnName, streamID)
		}
	}
	return keys, nil
}

func queryDiscoverComputedColumns(opts discoveryOptions) string {
	if opts.DiscoverOnlyEnabled {
		return `
SELECT DISTINCT sch.name, tbl.name, col.name
  FROM sys.change_tracking_tables ctt
    JOIN sys.tables tbl ON ctt.object_id = tbl.object_id
	JOIN sys.schemas sch ON sch.schema_id = tbl.schema_id
	JOIN sys.columns col ON col.object_id = tbl.object_id
  WHERE col.is_computed = 1;`
	}
	return `
SELECT sch.name, tbl.name, col.name
  FROM sys.columns col
    JOIN sys.tables tbl ON tbl.object_id = col.object_id
	JOIN sys.schemas sch ON sch.schema_id = tbl.schema_id
  WHERE col.is_computed = 1;`
}

func getComputedColumns(ctx context.Context, conn *sql.DB, opts discoveryOptions) (map[sqlcapture.StreamID][]string, error) {
	var rows, err = conn.QueryContext(ctx, queryDiscoverComputedColumns(opts))
	if err != nil {
		return nil, fmt.Errorf("error querying computed columns: %w", err)
	}
	defer rows.Close()

	var computedColumns = make(map[sqlcapture.StreamID][]string)
	for rows.Next() {
		var tableSchema, tableName, columnName string
		if err := rows.Scan(&tableSchema, &tableName, &columnName); err != nil {
			return nil, fmt.Errorf("error scanning result row: %w", err)
		}
		var streamID = sqlcapture.JoinStreamID(tableSchema, tableName)
		computedColumns[streamID] = append(computedColumns[streamID], columnName)
	}
	return computedColumns, nil
}

// TranslateDBToJSONType returns JSON schema information about the provided database column type.
func (db *sqlserverDatabase) TranslateDBToJSONType(column sqlcapture.ColumnInfo, isPrimaryKey bool) (*jsonschema.Schema, error) {
	var schema columnSchema
	if typeInfo, ok := column.DataType.(*sqlserverTextColumnType); ok {
		if schema, ok = db.databaseTypeToJSON(typeInfo.Type); !ok {
			return nil, fmt.Errorf("unhandled SQL Server type %q (found on column %q of table %q)", typeInfo.Type, column.Name, column.TableName)
		}
		// Add length constraints for char/varchar/nchar/nvarchar
		if typeInfo.MaxLength > 0 {
			switch typeInfo.Type {
			case "char", "nchar":
				// CHAR/NCHAR - fixed-length
				var length = uint64(typeInfo.MaxLength)
				// NOTE: In theory we might want to discover a minimum length for fixed-length
				// CHAR(n) columns, but in practice we have observed this minimum being violated
				// and there's no real benefit to having it right now, so we don't.
				schema.maxLength = &length
			case "varchar", "nvarchar":
				// VARCHAR/NVARCHAR - variable-length with limit
				var length = uint64(typeInfo.MaxLength)
				schema.maxLength = &length
			}
		}
	} else if typeInfo, ok := column.DataType.(*sqlserverBinaryColumnType); ok {
		if schema, ok = db.databaseTypeToJSON(typeInfo.Type); !ok {
			return nil, fmt.Errorf("unhandled SQL Server type %q (found on column %q of table %q)", typeInfo.Type, column.Name, column.TableName)
		}
		// Add length constraints for binary/varbinary
		// Binary data is base64 encoded: every 3 bytes becomes 4 characters
		if typeInfo.MaxLength > 0 {
			var base64Length = uint64((typeInfo.MaxLength + 2) / 3 * 4)
			switch typeInfo.Type {
			case "binary":
				// BINARY - fixed-length
				// NOTE: As with CHAR(n) we could theoretically discover a minimum here, and
				// we have not observed that minimum being violated in the real world for a
				// BINARY(n) column, but since there's no real benefit we choose not to at
				// this time.
				schema.maxLength = &base64Length
			case "varbinary":
				// VARBINARY - variable-length with limit
				schema.maxLength = &base64Length
			}
		}
	} else if typeName, ok := column.DataType.(string); ok {
		if schema, ok = db.databaseTypeToJSON(typeName); !ok {
			return nil, fmt.Errorf("unhandled SQL Server type %q (found on column %q of table %q)", typeName, column.Name, column.TableName)
		}
	} else {
		return nil, fmt.Errorf("unhandled SQL Server type %#v (found on column %q of table %q)", column.DataType, column.Name, column.TableName)
	}

	// Pass-through the column nullability and description.
	schema.nullable = column.IsNullable
	if column.Description != nil {
		schema.description = *column.Description
	}
	return schema.toType(), nil
}

type columnSchema struct {
	contentEncoding string
	description     string
	format          string
	nullable        bool
	extras          map[string]interface{}
	jsonType        string
	minLength       *uint64
	maxLength       *uint64
}

func (s columnSchema) toType() *jsonschema.Schema {
	var out = &jsonschema.Schema{
		Format:      s.format,
		Description: s.description,
		Extras:      make(map[string]interface{}),
		MinLength:   s.minLength,
		MaxLength:   s.maxLength,
	}
	for k, v := range s.extras {
		out.Extras[k] = v
	}

	if s.contentEncoding != "" {
		out.Extras["contentEncoding"] = s.contentEncoding // New in 2019-09.
	}

	if s.jsonType == "" {
		// No type constraint.
	} else if s.nullable {
		out.Extras["type"] = []string{s.jsonType, "null"} // Use variadic form.
	} else {
		out.Type = s.jsonType
	}
	return out
}

func (db *sqlserverDatabase) databaseTypeToJSON(typeName string) (columnSchema, bool) {
	var columnType, ok = sqlserverTypeToJSON[typeName]
	return columnType, ok
}

var sqlserverTypeToJSON = map[string]columnSchema{
	"bigint":   {jsonType: "integer"},
	"int":      {jsonType: "integer"},
	"smallint": {jsonType: "integer"},
	"tinyint":  {jsonType: "integer"},

	"numeric":    {jsonType: "string", format: "number"},
	"decimal":    {jsonType: "string", format: "number"},
	"money":      {jsonType: "string", format: "number"},
	"smallmoney": {jsonType: "string", format: "number"},

	"bit": {jsonType: "boolean"},

	"float": {jsonType: "number"},
	"real":  {jsonType: "number"},

	"char":     {jsonType: "string"},
	"varchar":  {jsonType: "string"},
	"text":     {jsonType: "string"},
	"nchar":    {jsonType: "string"},
	"nvarchar": {jsonType: "string"},
	"ntext":    {jsonType: "string"},

	"binary":    {jsonType: "string", contentEncoding: "base64"},
	"varbinary": {jsonType: "string", contentEncoding: "base64"},
	"image":     {jsonType: "string", contentEncoding: "base64"},

	// A 'timestamp' in SQL Server is not a timestamp as it's usually meant, it's
	// actually a monotonic integer ID and is also called 'rowversion'. These are
	// 8-byte binary values which we capture as base64-encoded strings.
	"timestamp": {jsonType: "string", contentEncoding: "base64"},

	"date":           {jsonType: "string", format: "date"},
	"datetimeoffset": {jsonType: "string", format: "date-time"},

	// The 'time' format in JSON schemas means the RFC3339 'full-time' grammar rule,
	// which includes a numeric timezone offset. The TIME column in SQL Server has
	// no associated timezone data, and it's not possible to unambiguously assign a
	// numeric timezone offset to these HH:MM:SS time values using the configured
	// datetime location (handwaving at one reason: how do we know if DST applies?).
	//
	// So we don't do that, and that's why TIME columns just get turned into strings
	// without a specific format guarantee here.
	"time": {jsonType: "string"},

	"uniqueidentifier": {jsonType: "string", format: "uuid"},

	"xml": {jsonType: "string"},

	"datetime":      {jsonType: "string", format: "date-time"},
	"datetime2":     {jsonType: "string", format: "date-time"},
	"smalldatetime": {jsonType: "string", format: "date-time"},

	"hierarchyid": {jsonType: "string", contentEncoding: "base64"},
}
