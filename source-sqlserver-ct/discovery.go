package main

import (
	"context"
	"database/sql"
	"fmt"
	"slices"
	"strings"

	"github.com/estuary/connectors/go/capture/sqlserver/datatypes"
	"github.com/estuary/connectors/sqlcapture"
	"github.com/invopop/jsonschema"
	log "github.com/sirupsen/logrus"
)

// discoveryOptions controls how discovery queries are constructed.
type discoveryOptions struct {
	DiscoverOnlyEnabled bool // Only discover tables with Change Tracking enabled
}

// DiscoverTables queries the database for information about tables available for capture.
func (db *sqlserverDatabase) DiscoverTables(ctx context.Context) (map[sqlcapture.StreamID]*sqlcapture.DiscoveryInfo, error) {
	var opts = discoveryOptions{
		DiscoverOnlyEnabled: !db.config.Advanced.DiscoverNonEnabled,
	}

	// Get lists of all tables, columns and primary keys in the database
	var tables, err = getTables(ctx, db.conn, opts)
	if err != nil {
		return nil, fmt.Errorf("unable to list database tables: %w", err)
	}
	columns, err := getColumns(ctx, db.conn, opts)
	if err != nil {
		return nil, fmt.Errorf("unable to list database columns: %w", err)
	}
	primaryKeys, err := getPrimaryKeys(ctx, db.conn, opts)
	if err != nil {
		return nil, fmt.Errorf("unable to list database primary keys: %w", err)
	}
	computedColumns, err := getComputedColumns(ctx, db.conn, opts)
	if err != nil {
		return nil, fmt.Errorf("unable to list database computed columns: %w", err)
	}

	// Aggregate column information into DiscoveryInfo structs using a map
	// from fully-qualified table names to the corresponding info.
	var tableMap = make(map[sqlcapture.StreamID]*sqlcapture.DiscoveryInfo)
	for _, table := range tables {
		var streamID = sqlcapture.JoinStreamID(table.Schema, table.Name)

		// Depending on feature flag settings, we may normalize multiple table names
		// to the same StreamID. This is a problem and other parts of discovery won't
		// be able to handle it gracefully, so it's a fatal error.
		if other, ok := tableMap[streamID]; ok {
			return nil, fmt.Errorf("table name collision between %q and %q",
				fmt.Sprintf("%s.%s", table.Schema, table.Name),
				fmt.Sprintf("%s.%s", other.Schema, other.Name),
			)
		}

		table.UseSchemaInference = true
		table.EmitSourcedSchemas = true
		tableMap[streamID] = table
	}
	for _, column := range columns {
		var streamID = sqlcapture.JoinStreamID(column.TableSchema, column.TableName)
		var info, ok = tableMap[streamID]
		if !ok {
			continue
		}

		if info.Columns == nil {
			info.Columns = make(map[string]sqlcapture.ColumnInfo)
		}
		info.Columns[column.Name] = column
		info.ColumnNames = append(info.ColumnNames, column.Name)
		tableMap[streamID] = info
	}

	// Add primary-key information to the tables map
	for id, key := range primaryKeys {
		var info, ok = tableMap[id]
		if !ok {
			continue
		}
		info.PrimaryKey = key
		tableMap[id] = info
	}

	// Add computed columns information (for informational purposes only - CT can capture computed columns)
	for streamID, table := range tableMap {
		if details, ok := table.ExtraDetails.(*sqlserverTableDiscoveryDetails); ok {
			details.ComputedColumns = computedColumns[streamID]
		}
	}

	// Determine whether the database sorts the keys of each table in a
	// predictable order or not. The term "predictable" here specifically
	// means "able to be reproduced using bytewise lexicographic ordering of
	// the serialized row keys generated by this connector".
	for _, info := range tableMap {
		for _, colName := range info.PrimaryKey {
			var dataType = info.Columns[colName].DataType
			if _, ok := dataType.(*datatypes.TextColumnType); ok {
				info.UnpredictableKeyOrdering = true
			} else if dataType == "numeric" || dataType == "decimal" {
				info.UnpredictableKeyOrdering = true
			}
		}
	}

	// Change Tracking requires primary keys. Mark tables without a primary key as omitted.
	for streamID, info := range tableMap {
		if len(info.PrimaryKey) == 0 {
			log.WithField("table", streamID).Debug("omitting table without primary key (required for Change Tracking)")
			info.OmitBinding = true
		}
	}

	if log.IsLevelEnabled(log.DebugLevel) {
		for id, info := range tableMap {
			log.WithFields(log.Fields{
				"stream":     id,
				"keyColumns": info.PrimaryKey,
			}).Debug("discovered table")
		}
	}

	return tableMap, nil
}

type sqlserverTableDiscoveryDetails struct {
	ComputedColumns []string // List of the names of computed columns in this table, in no particular order.
}

func queryDiscoverTables(opts discoveryOptions) string {
	if opts.DiscoverOnlyEnabled {
		return `
SELECT DISTINCT IST.TABLE_SCHEMA, IST.TABLE_NAME, IST.TABLE_TYPE
  FROM sys.change_tracking_tables ctt
  JOIN sys.tables tbl ON ctt.object_id = tbl.object_id
  JOIN sys.schemas sch ON tbl.schema_id = sch.schema_id
  JOIN INFORMATION_SCHEMA.TABLES IST
    ON IST.TABLE_SCHEMA = sch.name AND IST.TABLE_NAME = tbl.name
  WHERE IST.TABLE_SCHEMA NOT IN ('INFORMATION_SCHEMA', 'PERFORMANCE_SCHEMA', 'SYS')
    AND IST.TABLE_NAME != 'SYSTRANSCHEMAS';`
	}
	return `
  SELECT TABLE_SCHEMA, TABLE_NAME, TABLE_TYPE
  FROM INFORMATION_SCHEMA.TABLES
  WHERE TABLE_SCHEMA != 'INFORMATION_SCHEMA' AND TABLE_SCHEMA != 'PERFORMANCE_SCHEMA'
    AND TABLE_SCHEMA != 'SYS'
	AND TABLE_NAME != 'SYSTRANSCHEMAS';`
}

func getTables(ctx context.Context, conn *sql.DB, opts discoveryOptions) ([]*sqlcapture.DiscoveryInfo, error) {
	rows, err := conn.QueryContext(ctx, queryDiscoverTables(opts))
	if err != nil {
		return nil, fmt.Errorf("error listing tables: %w", err)
	}
	defer rows.Close()

	var tables []*sqlcapture.DiscoveryInfo
	for rows.Next() {
		var tableSchema, tableName, tableType string
		if err := rows.Scan(&tableSchema, &tableName, &tableType); err != nil {
			return nil, fmt.Errorf("error scanning result row: %w", err)
		}
		tables = append(tables, &sqlcapture.DiscoveryInfo{
			Schema:       tableSchema,
			Name:         tableName,
			BaseTable:    strings.EqualFold(tableType, "BASE TABLE"),
			ExtraDetails: &sqlserverTableDiscoveryDetails{},
		})
	}
	return tables, nil
}

func queryDiscoverColumns(opts discoveryOptions) string {
	if opts.DiscoverOnlyEnabled {
		return `
SELECT DISTINCT ISC.TABLE_SCHEMA, ISC.TABLE_NAME, ISC.ORDINAL_POSITION, ISC.COLUMN_NAME,
       ISC.IS_NULLABLE, ISC.DATA_TYPE, ISC.COLLATION_NAME, ISC.CHARACTER_MAXIMUM_LENGTH
  FROM sys.change_tracking_tables ctt
  JOIN sys.tables tbl ON ctt.object_id = tbl.object_id
  JOIN sys.schemas sch ON tbl.schema_id = sch.schema_id
  JOIN INFORMATION_SCHEMA.COLUMNS ISC
    ON ISC.TABLE_SCHEMA = sch.name AND ISC.TABLE_NAME = tbl.name
  ORDER BY ISC.TABLE_SCHEMA, ISC.TABLE_NAME, ISC.ORDINAL_POSITION;`
	}
	return `
  SELECT TABLE_SCHEMA, TABLE_NAME, ORDINAL_POSITION, COLUMN_NAME, IS_NULLABLE, DATA_TYPE, COLLATION_NAME, CHARACTER_MAXIMUM_LENGTH
  FROM INFORMATION_SCHEMA.COLUMNS
  ORDER BY TABLE_SCHEMA, TABLE_NAME, ORDINAL_POSITION;`
}

func getColumns(ctx context.Context, conn *sql.DB, opts discoveryOptions) ([]sqlcapture.ColumnInfo, error) {
	var rows, err = conn.QueryContext(ctx, queryDiscoverColumns(opts))
	if err != nil {
		return nil, fmt.Errorf("error querying columns: %w", err)
	}
	defer rows.Close()

	var columns []sqlcapture.ColumnInfo
	for rows.Next() {
		var ci sqlcapture.ColumnInfo
		var isNullable, typeName string
		var collationName *string
		var maxCharLength *int
		if err := rows.Scan(&ci.TableSchema, &ci.TableName, &ci.Index, &ci.Name, &isNullable, &typeName, &collationName, &maxCharLength); err != nil {
			return nil, fmt.Errorf("error scanning result row: %w", err)
		}
		ci.IsNullable = isNullable != "NO"
		if slices.Contains([]string{"char", "varchar", "nchar", "nvarchar", "text", "ntext"}, typeName) {
			// The collation name should never be null for a text column type, but if it
			// is we'll just default to the string 'NULL' so it's clear what the error is.
			var collation = "NULL"
			if collationName != nil {
				collation = *collationName
			}
			var fullType = typeName
			var columnSize int = 64 // For historical reasons we want to overestimate if CHAR_MAX_LENGTH is null
			if maxCharLength != nil {
				columnSize = *maxCharLength
			}
			if slices.Contains([]string{"char", "varchar", "nchar", "nvarchar"}, typeName) {
				fullType = fmt.Sprintf("%s(%d)", typeName, columnSize)
			}
			ci.DataType = &datatypes.TextColumnType{
				Type:      typeName,
				Collation: collation,
				FullType:  fullType,
				MaxLength: columnSize,
			}
		} else if slices.Contains([]string{"binary", "varbinary"}, typeName) {
			var columnSize int
			if maxCharLength != nil {
				columnSize = *maxCharLength
			}
			ci.DataType = &datatypes.BinaryColumnType{
				Type:      typeName,
				MaxLength: columnSize,
			}
		} else {
			ci.DataType = typeName
		}
		columns = append(columns, ci)
	}
	return columns, nil
}

// Joining on the 6-tuple {CONSTRAINT,TABLE}_{CATALOG,SCHEMA,NAME} is probably
// overkill but shouldn't hurt, and helps to make absolutely sure that we're
// matching up the constraint type with the column names/positions correctly.
func queryDiscoverPrimaryKeys(opts discoveryOptions) string {
	if opts.DiscoverOnlyEnabled {
		return `
SELECT DISTINCT KCU.TABLE_SCHEMA, KCU.TABLE_NAME, KCU.COLUMN_NAME, KCU.ORDINAL_POSITION
  FROM sys.change_tracking_tables ctt
  JOIN sys.tables tbl ON ctt.object_id = tbl.object_id
  JOIN sys.schemas sch ON tbl.schema_id = sch.schema_id
  JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE KCU
    ON KCU.TABLE_SCHEMA = sch.name AND KCU.TABLE_NAME = tbl.name
  JOIN INFORMATION_SCHEMA.TABLE_CONSTRAINTS TCS
    ON  TCS.CONSTRAINT_CATALOG = KCU.CONSTRAINT_CATALOG
    AND TCS.CONSTRAINT_SCHEMA = KCU.CONSTRAINT_SCHEMA
    AND TCS.CONSTRAINT_NAME = KCU.CONSTRAINT_NAME
    AND TCS.TABLE_CATALOG = KCU.TABLE_CATALOG
    AND TCS.TABLE_SCHEMA = KCU.TABLE_SCHEMA
    AND TCS.TABLE_NAME = KCU.TABLE_NAME
  WHERE TCS.CONSTRAINT_TYPE = 'PRIMARY KEY'
  ORDER BY KCU.TABLE_SCHEMA, KCU.TABLE_NAME, KCU.ORDINAL_POSITION;`
	}
	return `
SELECT KCU.TABLE_SCHEMA, KCU.TABLE_NAME, KCU.COLUMN_NAME, KCU.ORDINAL_POSITION
  FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE KCU
  JOIN INFORMATION_SCHEMA.TABLE_CONSTRAINTS TCS
    ON  TCS.CONSTRAINT_CATALOG = KCU.CONSTRAINT_CATALOG
    AND TCS.CONSTRAINT_SCHEMA = KCU.CONSTRAINT_SCHEMA
    AND TCS.CONSTRAINT_NAME = KCU.CONSTRAINT_NAME
    AND TCS.TABLE_CATALOG = KCU.TABLE_CATALOG
    AND TCS.TABLE_SCHEMA = KCU.TABLE_SCHEMA
    AND TCS.TABLE_NAME = KCU.TABLE_NAME
  WHERE TCS.CONSTRAINT_TYPE = 'PRIMARY KEY'
  ORDER BY KCU.TABLE_SCHEMA, KCU.TABLE_NAME, KCU.ORDINAL_POSITION;`
}

func getPrimaryKeys(ctx context.Context, conn *sql.DB, opts discoveryOptions) (map[sqlcapture.StreamID][]string, error) {
	var rows, err = conn.QueryContext(ctx, queryDiscoverPrimaryKeys(opts))
	if err != nil {
		return nil, fmt.Errorf("error querying primary keys: %w", err)
	}
	defer rows.Close()

	var keys = make(map[sqlcapture.StreamID][]string)
	for rows.Next() {
		var tableSchema, tableName, columnName string
		var index int
		if err := rows.Scan(&tableSchema, &tableName, &columnName, &index); err != nil {
			return nil, fmt.Errorf("error scanning result row: %w", err)
		}
		var streamID = sqlcapture.JoinStreamID(tableSchema, tableName)
		keys[streamID] = append(keys[streamID], columnName)
		if index != len(keys[streamID]) {
			return nil, fmt.Errorf("primary key column %q (of table %q) appears out of order", columnName, streamID)
		}
	}
	return keys, nil
}

func queryDiscoverComputedColumns(opts discoveryOptions) string {
	if opts.DiscoverOnlyEnabled {
		return `
SELECT DISTINCT sch.name, tbl.name, col.name
  FROM sys.change_tracking_tables ctt
    JOIN sys.tables tbl ON ctt.object_id = tbl.object_id
	JOIN sys.schemas sch ON sch.schema_id = tbl.schema_id
	JOIN sys.columns col ON col.object_id = tbl.object_id
  WHERE col.is_computed = 1;`
	}
	return `
SELECT sch.name, tbl.name, col.name
  FROM sys.columns col
    JOIN sys.tables tbl ON tbl.object_id = col.object_id
	JOIN sys.schemas sch ON sch.schema_id = tbl.schema_id
  WHERE col.is_computed = 1;`
}

func getComputedColumns(ctx context.Context, conn *sql.DB, opts discoveryOptions) (map[sqlcapture.StreamID][]string, error) {
	var rows, err = conn.QueryContext(ctx, queryDiscoverComputedColumns(opts))
	if err != nil {
		return nil, fmt.Errorf("error querying computed columns: %w", err)
	}
	defer rows.Close()

	var computedColumns = make(map[sqlcapture.StreamID][]string)
	for rows.Next() {
		var tableSchema, tableName, columnName string
		if err := rows.Scan(&tableSchema, &tableName, &columnName); err != nil {
			return nil, fmt.Errorf("error scanning result row: %w", err)
		}
		var streamID = sqlcapture.JoinStreamID(tableSchema, tableName)
		computedColumns[streamID] = append(computedColumns[streamID], columnName)
	}
	return computedColumns, nil
}

// TranslateDBToJSONType returns JSON schema information about the provided database column type.
func (db *sqlserverDatabase) TranslateDBToJSONType(column sqlcapture.ColumnInfo, _isPrimaryKey bool) (*jsonschema.Schema, error) {
	return datatypes.TranslateDBToJSONType(db.datatypesConfig, column)
}
